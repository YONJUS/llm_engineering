{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a6ab9a2-28a2-445d-8512-a0dc8d1b54e9",
   "metadata": {},
   "source": [
    "# Code Docstring Generator\n",
    "\n",
    "Generate code Docstring of python code using both OpenAI and Qwen.\n",
    "First of all, giving model the coding problem, asked to output the code that solved it the fastest for each model with Docstring.\n",
    "And if press the Run Python button below, the code will run and the elapsed time will be printed.\n",
    "My box lacks memory to use \"Qwen/CodeQwen1.5-7B-Chat\" as it is. So I'm going to use quantization first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04235e23-f3ed-4d14-98fe-dcffc741d475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # If True, We can use GPU\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e610bf56-a46e-4aff-8de1-ab49d62b1ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display, update_display\n",
    "import gradio as gr\n",
    "import subprocess\n",
    "\n",
    "load_dotenv(override=True)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN', 'your-key-if-not-using-env')\n",
    "\n",
    "openai = OpenAI()\n",
    "OPENAI_MODEL = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "60628c75-8b9d-4163-911a-045936d1d190",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_code = \"You are an assistant that generates optimal solution of python code.\"\n",
    "system_message_code += \"Add Docstrings for analyzing the functions and classes and adding explanations of Python code based on the purpose or input/output information, except Test example.\"\n",
    "system_message_code += \"Respond only with Python code; do not provide any explanation outside of code.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e7b3546-57aa-4c29-bc5d-f211970d04eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompt_for_code(quesion, python):\n",
    "    user_prompt = \"Given the coding problem, Rewrite this Python code with time and memory optimalization that produces identical output in the least time. \"\n",
    "    user_prompt += \"Remember to include all necessary Python library and packages such as import numpy.\\n\\n\"\n",
    "    user_prompt += question\n",
    "    user_prompt += python\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaf97b6f-dfa8-4f03-a71a-1890285a301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '''머쓱이는 태어난 지 6개월 된 조카를 돌보고 있습니다. \n",
    "조카는 아직 \"aya\", \"ye\", \"woo\", \"ma\" 네 가지 발음을 최대 한 번씩 사용해 조합한(이어 붙인) 발음밖에 하지 못합니다. \n",
    "문자열 배열 babbling이 매개변수로 주어질 때,머쓱이의 조카가 발음할 수 있는 단어의 개수를 return하도록 solution 함수를 완성해주세요.'''\n",
    "\n",
    "python = '''\n",
    "from itertools import permutations\n",
    "def solution(babbling):\n",
    "    able_ans=[]  \n",
    "    for i in range(1,5):\n",
    "        res = list(permutations([\"aya\", \"ye\", \"woo\", \"ma\"], i))\n",
    "        able_ans.append(res)\n",
    "        \n",
    "    bab_array=[]\n",
    "    for item in able_ans:\n",
    "        if isinstance(item, tuple):\n",
    "            result.append(''.join(item))\n",
    "        elif isinstance(item, list):\n",
    "            for subitem in item:\n",
    "                if isinstance(subitem, tuple):\n",
    "                    bab_array.append(''.join(subitem))\n",
    "    for k in babbling:\n",
    "        bab_array.append(k)\n",
    "\n",
    "    return len(bab_array) - len(set(bab_array))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6190659-f54c-4951-bef4-4960f8e51cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for_code(question, python):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_message_code},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for_code(question, python)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71e1ba8c-5b05-4726-a9f3-8d8c6257350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to a file called optimized.cpp\n",
    "\n",
    "def write_output(py):\n",
    "    code = py.replace(\"```python\",\"\").replace(\"```\",\"\")\n",
    "    with open(\"optimized.py\", \"w\") as f:\n",
    "        f.write(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7d2fea8-74c6-4421-8f1e-0e76d5b201b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_gpt(question, python):    \n",
    "    stream = openai.chat.completions.create(model=OPENAI_MODEL, messages=messages_for_code(question, python), stream=True)\n",
    "    reply = \"\"\n",
    "    for chunk in stream:\n",
    "        fragment = chunk.choices[0].delta.content or \"\"\n",
    "        reply += fragment\n",
    "        print(fragment, end='', flush=True)\n",
    "    write_output(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0be9f47d-5213-4700-b0e2-d444c7c738c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_gpt(question, python):    \n",
    "    stream = openai.chat.completions.create(model=OPENAI_MODEL, messages=messages_for_code(question, python), stream=True)\n",
    "    reply = \"\"\n",
    "    for chunk in stream:\n",
    "        fragment = chunk.choices[0].delta.content or \"\"\n",
    "        reply += fragment\n",
    "        yield reply.replace('```python\\n','').replace('```','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb8c5b4e-ec51-4f21-b3f8-6aa94fede86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login, InferenceClient\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13347633-4606-4e38-9927-80c39e65c1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "hf_token = os.environ['HF_TOKEN']\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "668dcb67-293f-4f55-9519-0daad0c27a7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "#del model  # 기존에 있던 모델 삭제\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0908d71c-c232-4da8-8d01-fd941743ddce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d44f3fa73a3d4cd2859c9157c686d467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# local에서 Qwen/CodeQwen1.5-7B-Chat모델 사용하기위해 4bit 양자화. 기존 14GB인데 8GB스펙이라 4GB로 사용하기 위함. 위에께 수정한거 이게 원본\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "code_qwen = \"Qwen/CodeQwen1.5-7B-Chat\"\n",
    "model_id = code_qwen\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    #device_map=\"auto\",  # 자동으로 GPU 할당\n",
    "    load_in_4bit=True,  # 4비트 양자화\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2ef8c663-4cba-4705-8eca-0686b1abc09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qwen_manual_prompt(system_message_code, user_message):\n",
    "    return (\n",
    "        f\"<|im_start|>system\\n{system_message_code}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>user\\n{user_message}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>assistant\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0dcb413c-a38f-40de-b8f1-53b615a10e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import re\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def run_code_qwen(question, python):\n",
    "    messages = messages_for_code(question, python)\n",
    "    prompt = qwen_manual_prompt(system_message_code, user_prompt_for_code(question, python))\n",
    "    #prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    if \"token_type_ids\" in inputs:\n",
    "        del inputs[\"token_type_ids\"]\n",
    "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "    output_ids = model.generate(**inputs, max_new_tokens=3000)\n",
    "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "def extract_python_code(response: str) -> str:\n",
    "    # ```python과 ``` 사이 텍스트 추출 (```python 포함하지 않음)\n",
    "    match = re.search(r\"```python\\s*(.*?)\\s*```\", response, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return \"No Python code found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bef39ff-f7a6-409c-8b3e-61af1e53b1c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def optimize(question, python, selected_model):\n",
    "    if selected_model == \"GPT\":\n",
    "        generator = stream_gpt(question, python)\n",
    "        for stream_so_far in generator:\n",
    "            yield stream_so_far\n",
    "        \n",
    "    elif selected_model == \"QWEN\":\n",
    "        response = run_code_qwen(question, python)\n",
    "        generator = extract_python_code(response)\n",
    "        yield generator\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe61f9d-118d-4de6-8940-308565d8b88c",
   "metadata": {},
   "source": [
    "## Adding execute time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5b5536a7-1644-491e-969d-f88e6090a2ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def execute_python(code):\n",
    "    try:\n",
    "        output = io.StringIO()\n",
    "        sys.stdout = output\n",
    "        start_time = time.time()\n",
    "        exec(code)\n",
    "        end_time = time.time()\n",
    "        print(\"Execution Time: {:.6f} seconds\".format(end_time - start_time))\n",
    "    finally:\n",
    "        sys.stdout = sys.__stdout__\n",
    "    return output.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9a1c7782-46a9-41e3-87ec-348d6e357dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "css = \"\"\"\n",
    ".python {background-color: #306998;}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "75fac8bb-a03a-4fda-870e-8af83454276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "#gradio with executation\n",
    "with gr.Blocks() as ui:\n",
    "    with gr.Row():\n",
    "        que = gr.Textbox(label=\"Question:\", lines=10 ,value=question)\n",
    "        py = gr.Textbox(label=\"Original Python code:\", lines=10, value=python)\n",
    "        result = gr.Textbox(label=\"Optimized code:\", lines=10)\n",
    "    with gr.Row():\n",
    "        selected_model = gr.Dropdown([\"GPT\", \"QWEN\"], label=\"Select model\", value=\"GPT\")\n",
    "        convert = gr.Button(\"Convert code\")\n",
    "    with gr.Row():\n",
    "        python_run = gr.Button(\"Run Python\")\n",
    "    with gr.Row():\n",
    "        python_out = gr.TextArea(label=\"Python result:\", elem_classes=[\"python\"])\n",
    "        \n",
    "    python_run.click(execute_python, inputs=[result], outputs=[python_out])   \n",
    "    convert.click(optimize, inputs=[que, py, selected_model], outputs=[result])\n",
    "    \n",
    "# gradio without excutation\n",
    "# with gr.Blocks() as ui:\n",
    "#     with gr.Row():\n",
    "#         que = gr.Textbox(label=\"Question:\", lines=10 ,value=question)\n",
    "#         py = gr.Textbox(label=\"Original Python code:\", lines=10, value=python)\n",
    "#         result = gr.Textbox(label=\"Optimized code:\", lines=10)\n",
    "#     with gr.Row():\n",
    "#         selected_model = gr.Dropdown([\"GPT\", \"QWEN\"], label=\"Select model\", value=\"GPT\")\n",
    "#         convert = gr.Button(\"Convert code\")\n",
    "    \n",
    "#     convert.click(optimize, inputs=[que, py, selected_model], outputs=[result])\n",
    "\n",
    "#ui.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84afc9e8-4d4e-4aed-8dbe-5b6e33315a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 2136, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 1674, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 729, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 723, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 706, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 867, in gen_wrapper\n",
      "    response = next(iterator)\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\AppData\\Local\\Temp\\ipykernel_20680\\1301357975.py\", line 8, in optimize\n",
      "    response = run_code_qwen(question, python)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\AppData\\Local\\Temp\\ipykernel_20680\\1313170791.py\", line 7, in run_code_qwen\n",
      "    prompt = tokenizer.apply_chat_template(messages=messages, tokenize=False, add_generation_prompt=True)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: PreTrainedTokenizerBase.apply_chat_template() missing 1 required positional argument: 'conversation'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 2136, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 1674, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 729, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 723, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 706, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 867, in gen_wrapper\n",
      "    response = next(iterator)\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\AppData\\Local\\Temp\\ipykernel_20680\\1301357975.py\", line 8, in optimize\n",
      "    response = run_code_qwen(question, python)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\AppData\\Local\\Temp\\ipykernel_20680\\684677845.py\", line 7, in run_code_qwen\n",
      "    prompt = qwen_manual_prompt(system_message_code, user_prompt_for_code(question, python))\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\AppData\\Local\\Temp\\ipykernel_20680\\145236784.py\", line 4, in qwen_manual_prompt\n",
      "    f\"<|im_start|>user\\n{user_message}<|im_end|>\\n\"\n",
      "                         ^^^^^^^^^^^^\n",
      "NameError: name 'user_message' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 2136, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 1674, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 729, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 723, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 706, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 867, in gen_wrapper\n",
      "    response = next(iterator)\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\AppData\\Local\\Temp\\ipykernel_20680\\1301357975.py\", line 8, in optimize\n",
      "    response = run_code_qwen(question, python)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\AppData\\Local\\Temp\\ipykernel_20680\\684677845.py\", line 7, in run_code_qwen\n",
      "    prompt = qwen_manual_prompt(system_message_code, user_prompt_for_code(question, python))\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\AppData\\Local\\Temp\\ipykernel_20680\\145236784.py\", line 4, in qwen_manual_prompt\n",
      "    f\"<|im_start|>user\\n{user_message}<|im_end|>\\n\"\n",
      "                         ^^^^^^^^^^^^\n",
      "NameError: name 'user_message' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 2136, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 1674, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 729, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 723, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 706, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 867, in gen_wrapper\n",
      "    response = next(iterator)\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\AppData\\Local\\Temp\\ipykernel_20680\\1301357975.py\", line 8, in optimize\n",
      "    response = run_code_qwen(question, python)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\AppData\\Local\\Temp\\ipykernel_20680\\684677845.py\", line 7, in run_code_qwen\n",
      "    prompt = qwen_manual_prompt(system_message_code, user_prompt_for_code(question, python))\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\AppData\\Local\\Temp\\ipykernel_20680\\145236784.py\", line 4, in qwen_manual_prompt\n",
      "    f\"<|im_start|>user\\n{user_message}<|im_end|>\\n\"\n",
      "                         ^^^^^^^^^^^^\n",
      "NameError: name 'user_message' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 2136, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 1662, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 884, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\AppData\\Local\\Temp\\ipykernel_20680\\922155633.py\", line 8, in execute_python\n",
      "    exec(code)\n",
      "  File \"<string>\", line 28, in <module>\n",
      "AssertionError\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 2136, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 1662, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 884, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\AppData\\Local\\Temp\\ipykernel_20680\\922155633.py\", line 8, in execute_python\n",
      "    exec(code)\n",
      "  File \"<string>\", line 28, in <module>\n",
      "AssertionError\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 2136, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 1662, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 884, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\AppData\\Local\\Temp\\ipykernel_20680\\922155633.py\", line 8, in execute_python\n",
      "    exec(code)\n",
      "  File \"<string>\", line 28, in <module>\n",
      "AssertionError\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 2136, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 1662, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 884, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\AppData\\Local\\Temp\\ipykernel_20680\\922155633.py\", line 8, in execute_python\n",
      "    exec(code)\n",
      "  File \"<string>\", line 28, in <module>\n",
      "AssertionError\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 2136, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 1662, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 884, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\AppData\\Local\\Temp\\ipykernel_20680\\922155633.py\", line 8, in execute_python\n",
      "    exec(code)\n",
      "  File \"<string>\", line 28, in <module>\n",
      "AssertionError\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 2136, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 1662, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 884, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\AppData\\Local\\Temp\\ipykernel_20680\\922155633.py\", line 8, in execute_python\n",
      "    exec(code)\n",
      "  File \"<string>\", line 22\n",
      "    valid_words.update([a + b + c\n",
      "                       ^\n",
      "SyntaxError: '[' was never closed\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 2136, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 1662, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 884, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\AppData\\Local\\Temp\\ipykernel_20680\\922155633.py\", line 8, in execute_python\n",
      "    exec(code)\n",
      "  File \"<string>\", line 22\n",
      "    valid_words.update([a + b + c for a in valid_syllables for b in valid_syllables for c in valid_syll\n",
      "                       ^\n",
      "SyntaxError: '[' was never closed\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 2136, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 1662, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 884, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\AppData\\Local\\Temp\\ipykernel_20680\\922155633.py\", line 8, in execute_python\n",
      "    exec(code)\n",
      "  File \"<string>\", line 22\n",
      "    valid_words.update([a + b + c for a in valid_syllables for b in valid_syllables for c in valid_syllables if len({a,\n",
      "                                                                                                                    ^\n",
      "SyntaxError: '{' was never closed\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 2136, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 1662, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 884, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\AppData\\Local\\Temp\\ipykernel_20680\\922155633.py\", line 8, in execute_python\n",
      "    exec(code)\n",
      "  File \"<string>\", line 23\n",
      "    valid_words.update([\"\".join(seq) for seq in permutations(valid_syllables, 4\n",
      "                                                            ^\n",
      "SyntaxError: '(' was never closed\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 2136, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 1662, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 884, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\AppData\\Local\\Temp\\ipykernel_20680\\922155633.py\", line 8, in execute_python\n",
      "    exec(code)\n",
      "  File \"<string>\", line 26\n",
      "    return sum(1 for word in\n",
      "              ^\n",
      "SyntaxError: '(' was never closed\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 2136, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 1662, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 884, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\82103\\AppData\\Local\\Temp\\ipykernel_20680\\922155633.py\", line 8, in execute_python\n",
      "    exec(code)\n",
      "  File \"<string>\", line 19\n",
      "    \n",
      "    ^\n",
      "IndentationError: expected an indented block after 'for' statement on line 18\n"
     ]
    }
   ],
   "source": [
    "ui.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0ad093-425b-488e-8c3f-67f729dd9c06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
